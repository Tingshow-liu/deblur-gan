{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://github.com/KupynOrest/DeblurGAN"
      ],
      "metadata": {
        "id": "aE0nhMB4lMuT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjAKwYGZBtYg",
        "outputId": "b61ee888-dcf0-4ba9-d716-ca32e4e062b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1frzgOHPrw0RnOGTtnrLA8W7rjJox1E3T\n",
            "From (redirected): https://drive.google.com/uc?id=1frzgOHPrw0RnOGTtnrLA8W7rjJox1E3T&confirm=t&uuid=97aa7c32-3f01-4207-9ad9-2bca6af27e30\n",
            "To: /content/blurred_sharp.zip\n",
            "100% 2.28G/2.28G [00:23<00:00, 98.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown\n",
        "!gdown --id 1frzgOHPrw0RnOGTtnrLA8W7rjJox1E3T"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upzip the file\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"blurred_sharp.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content\")  # /content/blurred_sharp"
      ],
      "metadata": {
        "id": "ijCLPaMMFD9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split data"
      ],
      "metadata": {
        "id": "ARPtgruFGvm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "JZNo7Da8Fkbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "base_dir = \"/content/blurred_sharp\"\n",
        "blurred_dir = os.path.join(base_dir, \"blurred\")\n",
        "sharp_dir = os.path.join(base_dir, \"sharp\")\n",
        "\n",
        "# New directories for splits\n",
        "splits = [\"train\", \"val\", \"test\"]\n",
        "for split in splits:\n",
        "    os.makedirs(os.path.join(base_dir, split, \"blurred\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(base_dir, split, \"sharp\"), exist_ok=True)\n",
        "\n",
        "# Get all file names\n",
        "blurred_files = sorted(os.listdir(blurred_dir))\n",
        "sharp_files = sorted(os.listdir(sharp_dir))\n",
        "\n",
        "# Ensure both directories contain the same number of files with matching names\n",
        "assert len(blurred_files) == len(sharp_files), \"Blurred and sharp directories must have the same number of files!\"\n",
        "for b, s in zip(blurred_files, sharp_files):\n",
        "    assert b == s, f\"File names do not match: {b} and {s}\"\n",
        "\n",
        "# Split into train, val, and test\n",
        "train_blurred, temp_blurred, train_sharp, temp_sharp = train_test_split(\n",
        "    blurred_files, sharp_files, test_size=0.3, random_state=42\n",
        ")\n",
        "val_blurred, test_blurred, val_sharp, test_sharp = train_test_split(\n",
        "    temp_blurred, temp_sharp, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Function to copy files to new directories\n",
        "def copy_files(file_list, source_dir, target_dir):\n",
        "    for file in file_list:\n",
        "        shutil.copy(os.path.join(source_dir, file), os.path.join(target_dir, file))\n",
        "\n",
        "# Copy files to corresponding directories\n",
        "copy_files(train_blurred, blurred_dir, os.path.join(base_dir, \"train/blurred\"))\n",
        "copy_files(train_sharp, sharp_dir, os.path.join(base_dir, \"train/sharp\"))\n",
        "copy_files(val_blurred, blurred_dir, os.path.join(base_dir, \"val/blurred\"))\n",
        "copy_files(val_sharp, sharp_dir, os.path.join(base_dir, \"val/sharp\"))\n",
        "copy_files(test_blurred, blurred_dir, os.path.join(base_dir, \"test/blurred\"))\n",
        "copy_files(test_sharp, sharp_dir, os.path.join(base_dir, \"test/sharp\"))\n",
        "\n",
        "print(\"Dataset successfully split into train, val, and test sets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49HRaZ2nF7kz",
        "outputId": "ee6a69d1-1920-4bc7-b2fe-cc756f23342e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset successfully split into train, val, and test sets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare data"
      ],
      "metadata": {
        "id": "3IolvxFuHcVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FbwQvovmTIJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Define transforms\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Resize all images to 128x128\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
        "])"
      ],
      "metadata": {
        "id": "aJtpYPr_HUCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DeblurDataset(Dataset):\n",
        "    def __init__(self, blurred_dir, sharp_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            blurred_dir (str): Path to the directory containing blurred images.\n",
        "            sharp_dir (str): Path to the directory containing sharp images.\n",
        "            transform (callable, optional): Optional transform to be applied to the images.\n",
        "        \"\"\"\n",
        "        self.blurred_dir = blurred_dir\n",
        "        self.sharp_dir = sharp_dir\n",
        "        self.transform = transform\n",
        "        self.image_names = sorted(os.listdir(blurred_dir))  # Ensure sorted order\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        blurred_path = os.path.join(self.blurred_dir, self.image_names[idx])\n",
        "        sharp_path = os.path.join(self.sharp_dir, self.image_names[idx])\n",
        "\n",
        "        # Load images\n",
        "        blurred_image = Image.open(blurred_path).convert(\"RGB\")\n",
        "        sharp_image = Image.open(sharp_path).convert(\"RGB\")\n",
        "\n",
        "        # Apply transformations if provided\n",
        "        if self.transform:\n",
        "            blurred_image = self.transform(blurred_image)\n",
        "            sharp_image = self.transform(sharp_image)\n",
        "\n",
        "        return blurred_image, sharp_image"
      ],
      "metadata": {
        "id": "yuygBUL3ICil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Paths to dataset\n",
        "train_blurred_dir = \"/content/blurred_sharp/train/blurred\"\n",
        "train_sharp_dir = \"/content/blurred_sharp/train/sharp\"\n",
        "val_blurred_dir = \"/content/blurred_sharp/val/blurred\"\n",
        "val_sharp_dir = \"/content/blurred_sharp/val/sharp\"\n",
        "test_blurred_dir = \"/content/blurred_sharp/test/blurred\"\n",
        "test_sharp_dir = \"/content/blurred_sharp/test/sharp\"\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = DeblurDataset(train_blurred_dir, train_sharp_dir, transform=image_transform)\n",
        "val_dataset = DeblurDataset(val_blurred_dir, val_sharp_dir, transform=image_transform)\n",
        "test_dataset = DeblurDataset(test_blurred_dir, test_sharp_dir, transform=image_transform)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "DQoRqrBhJrcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check\n",
        "for batch_idx, (blurred, sharp) in enumerate(train_loader):\n",
        "    print(f\"Batch {batch_idx + 1}:\")\n",
        "    print(f\"Blurred Image Tensor Shape: {blurred.shape}\")\n",
        "    print(f\"Sharp Image Tensor Shape: {sharp.shape}\")\n",
        "    print(f\"Blurred Image (first in batch): {blurred[0]}\")\n",
        "    print(f\"Sharp Image (first in batch): {sharp[0]}\")\n",
        "    break  # Stop after the first batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmQxNAO3KR_h",
        "outputId": "daf0b5b6-4932-4ea2-b5ae-232c010e5a88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1:\n",
            "Blurred Image Tensor Shape: torch.Size([16, 3, 128, 128])\n",
            "Sharp Image Tensor Shape: torch.Size([16, 3, 128, 128])\n",
            "Blurred Image (first in batch): tensor([[[ 0.2784,  0.2784,  0.2941,  ...,  0.5608,  0.4667,  0.3882],\n",
            "         [ 0.2078,  0.2157,  0.2235,  ...,  0.6157,  0.5294,  0.4431],\n",
            "         [ 0.1922,  0.1922,  0.2000,  ...,  0.6235,  0.5216,  0.4118],\n",
            "         ...,\n",
            "         [-0.7882, -0.7882, -0.8039,  ..., -0.8196, -0.8118, -0.8118],\n",
            "         [-0.7882, -0.7882, -0.7961,  ..., -0.8275, -0.8353, -0.8353],\n",
            "         [-0.7961, -0.7961, -0.7961,  ..., -0.8196, -0.8275, -0.8196]],\n",
            "\n",
            "        [[ 0.5686,  0.5765,  0.5765,  ...,  0.4902,  0.4902,  0.4902],\n",
            "         [ 0.5294,  0.5373,  0.5451,  ...,  0.5451,  0.5451,  0.5373],\n",
            "         [ 0.5137,  0.5216,  0.5294,  ...,  0.5608,  0.5451,  0.5216],\n",
            "         ...,\n",
            "         [-0.7804, -0.7804, -0.7882,  ..., -0.8118, -0.8039, -0.8039],\n",
            "         [-0.7804, -0.7804, -0.7804,  ..., -0.8275, -0.8353, -0.8353],\n",
            "         [-0.7882, -0.7882, -0.7804,  ..., -0.8118, -0.8196, -0.8118]],\n",
            "\n",
            "        [[ 0.8510,  0.8510,  0.8510,  ...,  0.4196,  0.5216,  0.6078],\n",
            "         [ 0.8275,  0.8275,  0.8353,  ...,  0.4667,  0.5608,  0.6392],\n",
            "         [ 0.8196,  0.8196,  0.8196,  ...,  0.4745,  0.5608,  0.6235],\n",
            "         ...,\n",
            "         [-0.7490, -0.7490, -0.7490,  ..., -0.7961, -0.7804, -0.7804],\n",
            "         [-0.7412, -0.7490, -0.7490,  ..., -0.8039, -0.8118, -0.8118],\n",
            "         [-0.7569, -0.7490, -0.7490,  ..., -0.7961, -0.8039, -0.7961]]])\n",
            "Sharp Image (first in batch): tensor([[[ 0.2784,  0.2863,  0.2863,  ...,  0.3961,  0.3255,  0.3176],\n",
            "         [ 0.2000,  0.2078,  0.2157,  ...,  0.4588,  0.3725,  0.3569],\n",
            "         [ 0.1765,  0.1765,  0.1922,  ...,  0.4510,  0.3176,  0.3098],\n",
            "         ...,\n",
            "         [-0.7961, -0.7961, -0.7961,  ..., -0.8824, -0.8275, -0.7804],\n",
            "         [-0.7882, -0.7882, -0.7882,  ..., -0.8196, -0.8745, -0.8431],\n",
            "         [-0.8039, -0.8118, -0.8196,  ..., -0.7882, -0.8588, -0.8980]],\n",
            "\n",
            "        [[ 0.5686,  0.5686,  0.5765,  ...,  0.5059,  0.4980,  0.4902],\n",
            "         [ 0.5294,  0.5373,  0.5373,  ...,  0.5608,  0.5373,  0.5294],\n",
            "         [ 0.5137,  0.5137,  0.5137,  ...,  0.5373,  0.4980,  0.4902],\n",
            "         ...,\n",
            "         [-0.7882, -0.7882, -0.7882,  ..., -0.8824, -0.8196, -0.7647],\n",
            "         [-0.7804, -0.7804, -0.7804,  ..., -0.8196, -0.8667, -0.8431],\n",
            "         [-0.7961, -0.8039, -0.8118,  ..., -0.7882, -0.8510, -0.8980]],\n",
            "\n",
            "        [[ 0.8510,  0.8510,  0.8588,  ...,  0.6078,  0.6706,  0.6706],\n",
            "         [ 0.8275,  0.8275,  0.8353,  ...,  0.6706,  0.7176,  0.7098],\n",
            "         [ 0.8118,  0.8118,  0.8118,  ...,  0.6392,  0.6784,  0.6784],\n",
            "         ...,\n",
            "         [-0.7412, -0.7412, -0.7412,  ..., -0.8588, -0.7961, -0.7333],\n",
            "         [-0.7412, -0.7412, -0.7333,  ..., -0.8039, -0.8431, -0.8118],\n",
            "         [-0.7569, -0.7647, -0.7725,  ..., -0.7804, -0.8353, -0.8745]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create model"
      ],
      "metadata": {
        "id": "7HJgtxGNL0Ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "OxrVNYRSNoDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discriminator"
      ],
      "metadata": {
        "id": "cNh1DxYrNfl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_nc=3, ndf=64, n_layers=3, use_sigmoid=False):\n",
        "        \"\"\"\n",
        "        Simplified multi-layer discriminator.\n",
        "        Args:\n",
        "            input_nc (int): Number of input channels (e.g., 3 for RGB images).\n",
        "            ndf (int): Number of filters in the first layer.\n",
        "            n_layers (int): Number of convolutional layers.\n",
        "            use_sigmoid (bool): Whether to apply a sigmoid activation in the final layer.\n",
        "        \"\"\"\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # Define the model layers\n",
        "        layers = []\n",
        "\n",
        "        # Initial layer\n",
        "        layers.append(\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(input_nc, ndf, kernel_size=4, stride=2, padding=1),\n",
        "                nn.LeakyReLU(0.2, inplace=True)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Intermediate layers\n",
        "        nf_mult = 1  # controls the number of feature maps (filters)\n",
        "        for n in range(1, n_layers):\n",
        "            nf_mult_prev = nf_mult\n",
        "            nf_mult = min(2**n, 8)\n",
        "            layers.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=4, stride=2, padding=1),\n",
        "                    nn.BatchNorm2d(ndf * nf_mult),\n",
        "                    nn.LeakyReLU(0.2, inplace=True)\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Final layer before output\n",
        "        nf_mult_prev = nf_mult\n",
        "        nf_mult = min(2**n_layers, 8)\n",
        "        layers.append(\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=4, stride=1, padding=1),\n",
        "                nn.BatchNorm2d(ndf * nf_mult),\n",
        "                nn.LeakyReLU(0.2, inplace=True)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        layers.append(nn.Conv2d(ndf * nf_mult, 1, kernel_size=4, stride=1, padding=1))\n",
        "        if use_sigmoid:\n",
        "            layers.append(nn.Sigmoid())\n",
        "\n",
        "        # Combine layers into a sequential model\n",
        "        self.model = nn.Sequential(*[layer for layer in layers])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "3AWlFjRQNhNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generator"
      ],
      "metadata": {
        "id": "iOz-p3JGP1tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResnetBlock(nn.Module):\n",
        "    \"\"\"Defines a single residual block.\"\"\"\n",
        "    def __init__(self, dim, padding_type='reflect', norm_layer=nn.BatchNorm2d, use_dropout=False, use_bias=True):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
        "\n",
        "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
        "        conv_block = []\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Padding type {padding_type} is not implemented.\")\n",
        "\n",
        "        conv_block += [\n",
        "            nn.Conv2d(dim, dim, kernel_size=3, bias=use_bias),\n",
        "            norm_layer(dim),\n",
        "            nn.ReLU(True)\n",
        "        ]\n",
        "        if use_dropout:\n",
        "            conv_block += [nn.Dropout(0.5)]\n",
        "\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "\n",
        "        conv_block += [\n",
        "            nn.Conv2d(dim, dim, kernel_size=3, bias=use_bias),\n",
        "            norm_layer(dim)\n",
        "        ]\n",
        "        return nn.Sequential(*conv_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_block(x)  # Add skip connection\n",
        "\n",
        "\n",
        "class ResnetGenerator(nn.Module):\n",
        "    def __init__(self, input_nc=3, output_nc=3, ngf=64, n_blocks=6):\n",
        "        \"\"\"\n",
        "        Simplified ResNet generator.\n",
        "        Args:\n",
        "            input_nc: Number of input channels (e.g., 3 for RGB).\n",
        "            output_nc: Number of output channels (e.g., 3 for RGB).\n",
        "            ngf: Number of filters in the first layer.\n",
        "            n_blocks: Number of ResNet blocks.\n",
        "        \"\"\"\n",
        "        super(ResnetGenerator, self).__init__()\n",
        "\n",
        "        # Initial convolution\n",
        "        model = [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True)\n",
        "        ]\n",
        "\n",
        "        # Downsampling\n",
        "        model += [\n",
        "            nn.Conv2d(ngf, ngf * 2, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ngf * 2, ngf * 4, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True)\n",
        "        ]\n",
        "\n",
        "        # ResNet blocks\n",
        "        for _ in range(n_blocks):\n",
        "            model += [ResnetBlock(ngf * 4)]\n",
        "\n",
        "        # Upsampling\n",
        "        model += [\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 2, ngf, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True)\n",
        "        ]\n",
        "\n",
        "        # Final convolution\n",
        "        model += [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0),\n",
        "            nn.Tanh()\n",
        "        ]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "-1lPqZ8IP3Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "iZIPRYhHjxZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train GAN"
      ],
      "metadata": {
        "id": "znKysohIQYAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Functions and Optimizers"
      ],
      "metadata": {
        "id": "_sC8b5ZlQe48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define Custom Loss Functions ---\n",
        "\n",
        "class AdversarialLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdversarialLoss, self).__init__()\n",
        "        self.criterion = nn.BCELoss()\n",
        "\n",
        "    def forward(self, predictions, is_real):\n",
        "        # Create labels based on whether the input is real or fake\n",
        "        target = torch.ones_like(predictions) if is_real else torch.zeros_like(predictions)\n",
        "        return self.criterion(predictions, target)\n",
        "\n",
        "class ContentLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ContentLoss, self).__init__()\n",
        "        self.criterion = nn.L1Loss()\n",
        "\n",
        "    def forward(self, fake_image, real_image):\n",
        "        return self.criterion(fake_image, real_image)\n",
        "\n",
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PerceptualLoss, self).__init__()\n",
        "        self.vgg = models.vgg19(pretrained=True).features[:14].eval()  # Up to conv3_3\n",
        "        for param in self.vgg.parameters():\n",
        "            param.requires_grad = False  # Freeze VGG\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def forward(self, fake_image, real_image):\n",
        "        fake_features = self.vgg(fake_image)\n",
        "        real_features = self.vgg(real_image)\n",
        "        return self.criterion(fake_features, real_features)\n",
        "\n",
        "class GANLoss:\n",
        "    def __init__(self, use_perceptual_loss=False):\n",
        "        self.adversarial_loss = AdversarialLoss()\n",
        "        self.content_loss = ContentLoss()\n",
        "        self.perceptual_loss = PerceptualLoss() if use_perceptual_loss else None\n",
        "\n",
        "    def generator_loss(self, discriminator, fake_image, real_image):\n",
        "        # Adversarial loss: fool the discriminator\n",
        "        g_fake_loss = self.adversarial_loss(discriminator(fake_image), is_real=True)\n",
        "\n",
        "        # Content loss: match the ground truth sharp image\n",
        "        g_content_loss = self.content_loss(fake_image, real_image)\n",
        "\n",
        "        # Optional: Perceptual loss\n",
        "        if self.perceptual_loss:\n",
        "            g_perceptual_loss = self.perceptual_loss(fake_image, real_image)\n",
        "        else:\n",
        "            g_perceptual_loss = 0\n",
        "\n",
        "        # Combine all generator losses\n",
        "        total_loss = g_fake_loss + 10 * g_content_loss + 0.1 * g_perceptual_loss\n",
        "        return total_loss\n",
        "\n",
        "    def discriminator_loss(self, discriminator, fake_image, real_image):\n",
        "        # Real images should be classified as real\n",
        "        d_real_loss = self.adversarial_loss(discriminator(real_image), is_real=True)\n",
        "\n",
        "        # Fake images should be classified as fake\n",
        "        d_fake_loss = self.adversarial_loss(discriminator(fake_image.detach()), is_real=False)\n",
        "\n",
        "        # Combine discriminator losses\n",
        "        total_loss = (d_real_loss + d_fake_loss) * 0.5\n",
        "        return total_loss"
      ],
      "metadata": {
        "id": "Aud_NPMNgiOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "# Initialize models\n",
        "generator = ResnetGenerator(input_nc=3, output_nc=3, ngf=64, n_blocks=6)\n",
        "discriminator = Discriminator(input_nc=3, ndf=64, n_layers=3, use_sigmoid=True)\n",
        "\n",
        "# Loss functions\n",
        "gan_loss = GANLoss(use_perceptual_loss=True)\n",
        "\n",
        "# Optimizers\n",
        "lr = 0.0002\n",
        "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Ensure all models and components are on the same device\n",
        "generator.to(device)\n",
        "discriminator.to(device)\n",
        "if gan_loss.perceptual_loss:\n",
        "    gan_loss.perceptual_loss.vgg.to(device)  # Move VGG for perceptual loss to GPU"
      ],
      "metadata": {
        "id": "0YVu8FYjgtct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = \"/content/\"\n",
        "\n",
        "# --- Training Loop ---\n",
        "num_epochs = 300\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (blurred, sharp) in enumerate(train_loader):\n",
        "        blurred, sharp = blurred.to(device), sharp.to(device)\n",
        "\n",
        "        # --- Train Discriminator ---\n",
        "        discriminator_optimizer.zero_grad()\n",
        "        fake_sharp = generator(blurred)\n",
        "        d_loss = gan_loss.discriminator_loss(discriminator, fake_sharp, sharp)\n",
        "        d_loss.backward()\n",
        "        discriminator_optimizer.step()\n",
        "\n",
        "        # --- Train Generator ---\n",
        "        generator_optimizer.zero_grad()\n",
        "        g_loss = gan_loss.generator_loss(discriminator, fake_sharp, sharp)\n",
        "        g_loss.backward()\n",
        "        generator_optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:  # Print progress every 100 batches\n",
        "            print(f\"[Epoch {epoch+1}/{num_epochs}] [Batch {i}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
        "\n",
        "    # Save the generator and discriminator models after each epoch\n",
        "    torch.save(generator.state_dict(), os.path.join(save_dir, f\"generator_epoch_{epoch+1}.pth\"))\n",
        "    torch.save(discriminator.state_dict(), os.path.join(save_dir, f\"discriminator_epoch_{epoch+1}.pth\"))\n",
        "    print(f\"Models saved for epoch {epoch+1}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSOdH5IHhFjX",
        "outputId": "1f2dd3f3-8b84-4d81-f0a9-ec6333ce4901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/300] [Batch 0] | D Loss: 0.7139 | G Loss: 7.4828\n",
            "Models saved for epoch 1.\n",
            "[Epoch 2/300] [Batch 0] | D Loss: 0.6881 | G Loss: 2.1054\n",
            "Models saved for epoch 2.\n",
            "[Epoch 3/300] [Batch 0] | D Loss: 0.6344 | G Loss: 2.3902\n",
            "Models saved for epoch 3.\n",
            "[Epoch 4/300] [Batch 0] | D Loss: 0.6673 | G Loss: 2.1518\n",
            "Models saved for epoch 4.\n",
            "[Epoch 5/300] [Batch 0] | D Loss: 0.7160 | G Loss: 2.3804\n",
            "Models saved for epoch 5.\n",
            "[Epoch 6/300] [Batch 0] | D Loss: 0.7050 | G Loss: 3.3896\n",
            "Models saved for epoch 6.\n",
            "[Epoch 7/300] [Batch 0] | D Loss: 0.6833 | G Loss: 1.6047\n",
            "Models saved for epoch 7.\n",
            "[Epoch 8/300] [Batch 0] | D Loss: 0.6801 | G Loss: 1.8144\n",
            "Models saved for epoch 8.\n",
            "[Epoch 9/300] [Batch 0] | D Loss: 0.6799 | G Loss: 1.6974\n",
            "Models saved for epoch 9.\n",
            "[Epoch 10/300] [Batch 0] | D Loss: 0.7119 | G Loss: 1.8714\n",
            "Models saved for epoch 10.\n",
            "[Epoch 11/300] [Batch 0] | D Loss: 0.6888 | G Loss: 1.9843\n",
            "Models saved for epoch 11.\n",
            "[Epoch 12/300] [Batch 0] | D Loss: 0.7020 | G Loss: 1.8846\n",
            "Models saved for epoch 12.\n",
            "[Epoch 13/300] [Batch 0] | D Loss: 0.6811 | G Loss: 1.9825\n",
            "Models saved for epoch 13.\n",
            "[Epoch 14/300] [Batch 0] | D Loss: 0.6836 | G Loss: 2.1476\n",
            "Models saved for epoch 14.\n",
            "[Epoch 15/300] [Batch 0] | D Loss: 0.6802 | G Loss: 2.1644\n",
            "Models saved for epoch 15.\n",
            "[Epoch 16/300] [Batch 0] | D Loss: 0.6583 | G Loss: 1.7170\n",
            "Models saved for epoch 16.\n",
            "[Epoch 17/300] [Batch 0] | D Loss: 0.6582 | G Loss: 1.6488\n",
            "Models saved for epoch 17.\n",
            "[Epoch 18/300] [Batch 0] | D Loss: 0.6786 | G Loss: 1.6933\n",
            "Models saved for epoch 18.\n",
            "[Epoch 19/300] [Batch 0] | D Loss: 0.6645 | G Loss: 1.5580\n",
            "Models saved for epoch 19.\n",
            "[Epoch 20/300] [Batch 0] | D Loss: 0.6917 | G Loss: 1.7700\n",
            "Models saved for epoch 20.\n",
            "[Epoch 21/300] [Batch 0] | D Loss: 1.0911 | G Loss: 2.2589\n",
            "Models saved for epoch 21.\n",
            "[Epoch 22/300] [Batch 0] | D Loss: 0.6909 | G Loss: 1.7409\n",
            "Models saved for epoch 22.\n",
            "[Epoch 23/300] [Batch 0] | D Loss: 0.6332 | G Loss: 1.7287\n",
            "Models saved for epoch 23.\n",
            "[Epoch 24/300] [Batch 0] | D Loss: 0.6873 | G Loss: 2.2736\n",
            "Models saved for epoch 24.\n",
            "[Epoch 25/300] [Batch 0] | D Loss: 0.7231 | G Loss: 1.7142\n",
            "Models saved for epoch 25.\n",
            "[Epoch 26/300] [Batch 0] | D Loss: 0.6476 | G Loss: 1.9564\n",
            "Models saved for epoch 26.\n",
            "[Epoch 27/300] [Batch 0] | D Loss: 0.6704 | G Loss: 1.8805\n",
            "Models saved for epoch 27.\n",
            "[Epoch 28/300] [Batch 0] | D Loss: 0.6996 | G Loss: 1.6495\n",
            "Models saved for epoch 28.\n",
            "[Epoch 29/300] [Batch 0] | D Loss: 0.6109 | G Loss: 1.9182\n",
            "Models saved for epoch 29.\n",
            "[Epoch 30/300] [Batch 0] | D Loss: 0.6699 | G Loss: 1.7649\n",
            "Models saved for epoch 30.\n",
            "[Epoch 31/300] [Batch 0] | D Loss: 0.5774 | G Loss: 1.7548\n",
            "Models saved for epoch 31.\n",
            "[Epoch 32/300] [Batch 0] | D Loss: 0.6260 | G Loss: 1.7937\n",
            "Models saved for epoch 32.\n",
            "[Epoch 33/300] [Batch 0] | D Loss: 0.6891 | G Loss: 1.5486\n",
            "Models saved for epoch 33.\n",
            "[Epoch 34/300] [Batch 0] | D Loss: 0.6525 | G Loss: 1.6855\n",
            "Models saved for epoch 34.\n",
            "[Epoch 35/300] [Batch 0] | D Loss: 0.6235 | G Loss: 1.6912\n",
            "Models saved for epoch 35.\n",
            "[Epoch 36/300] [Batch 0] | D Loss: 0.6580 | G Loss: 1.7428\n",
            "Models saved for epoch 36.\n",
            "[Epoch 37/300] [Batch 0] | D Loss: 0.6452 | G Loss: 1.8024\n",
            "Models saved for epoch 37.\n",
            "[Epoch 38/300] [Batch 0] | D Loss: 0.6505 | G Loss: 1.4473\n",
            "Models saved for epoch 38.\n",
            "[Epoch 39/300] [Batch 0] | D Loss: 0.6050 | G Loss: 2.2191\n",
            "Models saved for epoch 39.\n",
            "[Epoch 40/300] [Batch 0] | D Loss: 0.6544 | G Loss: 1.4329\n",
            "Models saved for epoch 40.\n",
            "[Epoch 41/300] [Batch 0] | D Loss: 0.6474 | G Loss: 1.5313\n",
            "Models saved for epoch 41.\n",
            "[Epoch 42/300] [Batch 0] | D Loss: 0.5758 | G Loss: 2.1843\n",
            "Models saved for epoch 42.\n",
            "[Epoch 43/300] [Batch 0] | D Loss: 0.6999 | G Loss: 1.4621\n",
            "Models saved for epoch 43.\n",
            "[Epoch 44/300] [Batch 0] | D Loss: 0.6349 | G Loss: 1.9818\n",
            "Models saved for epoch 44.\n",
            "[Epoch 45/300] [Batch 0] | D Loss: 0.6308 | G Loss: 2.1187\n",
            "Models saved for epoch 45.\n",
            "[Epoch 46/300] [Batch 0] | D Loss: 0.6206 | G Loss: 2.0051\n",
            "Models saved for epoch 46.\n",
            "[Epoch 47/300] [Batch 0] | D Loss: 0.5098 | G Loss: 2.5723\n",
            "Models saved for epoch 47.\n",
            "[Epoch 48/300] [Batch 0] | D Loss: 0.7581 | G Loss: 1.3779\n",
            "Models saved for epoch 48.\n",
            "[Epoch 49/300] [Batch 0] | D Loss: 0.6893 | G Loss: 1.3350\n",
            "Models saved for epoch 49.\n",
            "[Epoch 50/300] [Batch 0] | D Loss: 0.6664 | G Loss: 2.0004\n",
            "Models saved for epoch 50.\n",
            "[Epoch 51/300] [Batch 0] | D Loss: 0.7316 | G Loss: 1.3694\n",
            "Models saved for epoch 51.\n",
            "[Epoch 52/300] [Batch 0] | D Loss: 0.6434 | G Loss: 1.4446\n",
            "Models saved for epoch 52.\n",
            "[Epoch 53/300] [Batch 0] | D Loss: 0.6161 | G Loss: 1.4913\n",
            "Models saved for epoch 53.\n",
            "[Epoch 54/300] [Batch 0] | D Loss: 0.6452 | G Loss: 1.6280\n",
            "Models saved for epoch 54.\n",
            "[Epoch 55/300] [Batch 0] | D Loss: 0.5778 | G Loss: 1.6488\n",
            "Models saved for epoch 55.\n",
            "[Epoch 56/300] [Batch 0] | D Loss: 0.5988 | G Loss: 2.1724\n",
            "Models saved for epoch 56.\n",
            "[Epoch 57/300] [Batch 0] | D Loss: 0.5372 | G Loss: 2.1530\n",
            "Models saved for epoch 57.\n",
            "[Epoch 58/300] [Batch 0] | D Loss: 0.7659 | G Loss: 1.4576\n",
            "Models saved for epoch 58.\n",
            "[Epoch 59/300] [Batch 0] | D Loss: 0.6562 | G Loss: 1.4128\n",
            "Models saved for epoch 59.\n",
            "[Epoch 60/300] [Batch 0] | D Loss: 0.6641 | G Loss: 1.5964\n",
            "Models saved for epoch 60.\n",
            "[Epoch 61/300] [Batch 0] | D Loss: 0.6009 | G Loss: 1.7170\n",
            "Models saved for epoch 61.\n",
            "[Epoch 62/300] [Batch 0] | D Loss: 0.6004 | G Loss: 2.2514\n",
            "Models saved for epoch 62.\n",
            "[Epoch 63/300] [Batch 0] | D Loss: 0.6104 | G Loss: 1.6395\n",
            "Models saved for epoch 63.\n",
            "[Epoch 64/300] [Batch 0] | D Loss: 0.6018 | G Loss: 1.5639\n",
            "Models saved for epoch 64.\n",
            "[Epoch 65/300] [Batch 0] | D Loss: 0.5774 | G Loss: 1.8417\n",
            "Models saved for epoch 65.\n",
            "[Epoch 66/300] [Batch 0] | D Loss: 0.4712 | G Loss: 2.1900\n",
            "Models saved for epoch 66.\n",
            "[Epoch 67/300] [Batch 0] | D Loss: 0.9440 | G Loss: 1.5147\n",
            "Models saved for epoch 67.\n",
            "[Epoch 68/300] [Batch 0] | D Loss: 0.6963 | G Loss: 1.3842\n",
            "Models saved for epoch 68.\n",
            "[Epoch 69/300] [Batch 0] | D Loss: 0.6981 | G Loss: 1.3176\n",
            "Models saved for epoch 69.\n",
            "[Epoch 70/300] [Batch 0] | D Loss: 0.6942 | G Loss: 1.2600\n",
            "Models saved for epoch 70.\n",
            "[Epoch 71/300] [Batch 0] | D Loss: 0.7116 | G Loss: 1.4132\n",
            "Models saved for epoch 71.\n",
            "[Epoch 72/300] [Batch 0] | D Loss: 0.6952 | G Loss: 1.1918\n",
            "Models saved for epoch 72.\n",
            "[Epoch 73/300] [Batch 0] | D Loss: 0.6980 | G Loss: 1.3681\n",
            "Models saved for epoch 73.\n",
            "[Epoch 74/300] [Batch 0] | D Loss: 0.6993 | G Loss: 1.4992\n",
            "Models saved for epoch 74.\n",
            "[Epoch 75/300] [Batch 0] | D Loss: 0.6980 | G Loss: 1.1933\n",
            "Models saved for epoch 75.\n",
            "[Epoch 76/300] [Batch 0] | D Loss: 0.6893 | G Loss: 1.2946\n",
            "Models saved for epoch 76.\n",
            "[Epoch 77/300] [Batch 0] | D Loss: 0.6935 | G Loss: 1.4704\n",
            "Models saved for epoch 77.\n",
            "[Epoch 78/300] [Batch 0] | D Loss: 0.6968 | G Loss: 1.2531\n",
            "Models saved for epoch 78.\n",
            "[Epoch 79/300] [Batch 0] | D Loss: 0.6893 | G Loss: 1.3872\n",
            "Models saved for epoch 79.\n",
            "[Epoch 80/300] [Batch 0] | D Loss: 0.6969 | G Loss: 1.4066\n",
            "Models saved for epoch 80.\n",
            "[Epoch 81/300] [Batch 0] | D Loss: 0.6907 | G Loss: 1.2837\n",
            "Models saved for epoch 81.\n",
            "[Epoch 82/300] [Batch 0] | D Loss: 0.7026 | G Loss: 1.2789\n",
            "Models saved for epoch 82.\n",
            "[Epoch 83/300] [Batch 0] | D Loss: 0.7051 | G Loss: 1.7200\n",
            "Models saved for epoch 83.\n",
            "[Epoch 84/300] [Batch 0] | D Loss: 0.6848 | G Loss: 1.5965\n",
            "Models saved for epoch 84.\n",
            "[Epoch 85/300] [Batch 0] | D Loss: 0.6925 | G Loss: 1.2728\n",
            "Models saved for epoch 85.\n",
            "[Epoch 86/300] [Batch 0] | D Loss: 0.6959 | G Loss: 1.1362\n",
            "Models saved for epoch 86.\n",
            "[Epoch 87/300] [Batch 0] | D Loss: 0.6921 | G Loss: 1.1969\n",
            "Models saved for epoch 87.\n",
            "[Epoch 88/300] [Batch 0] | D Loss: 0.6963 | G Loss: 1.4728\n",
            "Models saved for epoch 88.\n",
            "[Epoch 89/300] [Batch 0] | D Loss: 0.6888 | G Loss: 1.3661\n",
            "Models saved for epoch 89.\n",
            "[Epoch 90/300] [Batch 0] | D Loss: 0.6858 | G Loss: 1.3117\n",
            "Models saved for epoch 90.\n",
            "[Epoch 91/300] [Batch 0] | D Loss: 0.7010 | G Loss: 1.3526\n",
            "Models saved for epoch 91.\n",
            "[Epoch 92/300] [Batch 0] | D Loss: 0.6930 | G Loss: 1.2872\n",
            "Models saved for epoch 92.\n",
            "[Epoch 93/300] [Batch 0] | D Loss: 0.6903 | G Loss: 1.5165\n",
            "Models saved for epoch 93.\n",
            "[Epoch 94/300] [Batch 0] | D Loss: 0.6969 | G Loss: 1.2567\n",
            "Models saved for epoch 94.\n",
            "[Epoch 95/300] [Batch 0] | D Loss: 0.7016 | G Loss: 1.7774\n",
            "Models saved for epoch 95.\n",
            "[Epoch 96/300] [Batch 0] | D Loss: 0.6947 | G Loss: 1.2137\n",
            "Models saved for epoch 96.\n",
            "[Epoch 97/300] [Batch 0] | D Loss: 0.6956 | G Loss: 1.2352\n",
            "Models saved for epoch 97.\n",
            "[Epoch 98/300] [Batch 0] | D Loss: 0.6879 | G Loss: 1.3635\n",
            "Models saved for epoch 98.\n",
            "[Epoch 99/300] [Batch 0] | D Loss: 0.7129 | G Loss: 1.6047\n",
            "Models saved for epoch 99.\n",
            "[Epoch 100/300] [Batch 0] | D Loss: 0.6942 | G Loss: 1.1456\n",
            "Models saved for epoch 100.\n",
            "[Epoch 101/300] [Batch 0] | D Loss: 0.6854 | G Loss: 1.2761\n",
            "Models saved for epoch 101.\n",
            "[Epoch 102/300] [Batch 0] | D Loss: 0.6887 | G Loss: 1.2754\n",
            "Models saved for epoch 102.\n",
            "[Epoch 103/300] [Batch 0] | D Loss: 0.6944 | G Loss: 1.2333\n",
            "Models saved for epoch 103.\n",
            "[Epoch 104/300] [Batch 0] | D Loss: 0.6875 | G Loss: 1.3184\n",
            "Models saved for epoch 104.\n",
            "[Epoch 105/300] [Batch 0] | D Loss: 0.7006 | G Loss: 1.2476\n",
            "Models saved for epoch 105.\n",
            "[Epoch 106/300] [Batch 0] | D Loss: 0.6972 | G Loss: 1.2262\n",
            "Models saved for epoch 106.\n",
            "[Epoch 107/300] [Batch 0] | D Loss: 0.6922 | G Loss: 1.1814\n",
            "Models saved for epoch 107.\n",
            "[Epoch 108/300] [Batch 0] | D Loss: 0.6943 | G Loss: 1.2136\n",
            "Models saved for epoch 108.\n",
            "[Epoch 109/300] [Batch 0] | D Loss: 0.6923 | G Loss: 1.2163\n",
            "Models saved for epoch 109.\n",
            "[Epoch 110/300] [Batch 0] | D Loss: 0.6858 | G Loss: 1.2091\n",
            "Models saved for epoch 110.\n",
            "[Epoch 111/300] [Batch 0] | D Loss: 0.6908 | G Loss: 1.2016\n",
            "Models saved for epoch 111.\n",
            "[Epoch 112/300] [Batch 0] | D Loss: 0.7093 | G Loss: 1.1146\n",
            "Models saved for epoch 112.\n",
            "[Epoch 113/300] [Batch 0] | D Loss: 0.7050 | G Loss: 1.2493\n",
            "Models saved for epoch 113.\n",
            "[Epoch 114/300] [Batch 0] | D Loss: 0.7380 | G Loss: 1.4281\n",
            "Models saved for epoch 114.\n",
            "[Epoch 115/300] [Batch 0] | D Loss: 0.6989 | G Loss: 1.1542\n",
            "Models saved for epoch 115.\n",
            "[Epoch 116/300] [Batch 0] | D Loss: 0.6956 | G Loss: 1.2618\n",
            "Models saved for epoch 116.\n",
            "[Epoch 117/300] [Batch 0] | D Loss: 0.6931 | G Loss: 1.2572\n",
            "Models saved for epoch 117.\n",
            "[Epoch 118/300] [Batch 0] | D Loss: 0.6929 | G Loss: 1.1923\n",
            "Models saved for epoch 118.\n",
            "[Epoch 119/300] [Batch 0] | D Loss: 0.6884 | G Loss: 1.1176\n",
            "Models saved for epoch 119.\n",
            "[Epoch 120/300] [Batch 0] | D Loss: 0.6919 | G Loss: 1.2671\n",
            "Models saved for epoch 120.\n",
            "[Epoch 121/300] [Batch 0] | D Loss: 0.6938 | G Loss: 1.1988\n",
            "Models saved for epoch 121.\n",
            "[Epoch 122/300] [Batch 0] | D Loss: 0.6853 | G Loss: 1.2799\n",
            "Models saved for epoch 122.\n",
            "[Epoch 123/300] [Batch 0] | D Loss: 0.7049 | G Loss: 1.1558\n",
            "Models saved for epoch 123.\n",
            "[Epoch 124/300] [Batch 0] | D Loss: 0.7046 | G Loss: 1.1812\n",
            "Models saved for epoch 124.\n",
            "[Epoch 125/300] [Batch 0] | D Loss: 0.6927 | G Loss: 1.1451\n",
            "Models saved for epoch 125.\n",
            "[Epoch 126/300] [Batch 0] | D Loss: 0.6907 | G Loss: 1.2023\n",
            "Models saved for epoch 126.\n",
            "[Epoch 127/300] [Batch 0] | D Loss: 0.6958 | G Loss: 1.0805\n",
            "Models saved for epoch 127.\n",
            "[Epoch 128/300] [Batch 0] | D Loss: 0.6905 | G Loss: 1.2568\n",
            "Models saved for epoch 128.\n",
            "[Epoch 129/300] [Batch 0] | D Loss: 0.6889 | G Loss: 1.1644\n",
            "Models saved for epoch 129.\n",
            "[Epoch 130/300] [Batch 0] | D Loss: 0.7139 | G Loss: 1.1258\n",
            "Models saved for epoch 130.\n",
            "[Epoch 131/300] [Batch 0] | D Loss: 0.6964 | G Loss: 1.2101\n",
            "Models saved for epoch 131.\n",
            "[Epoch 132/300] [Batch 0] | D Loss: 0.6845 | G Loss: 1.1746\n",
            "Models saved for epoch 132.\n",
            "[Epoch 133/300] [Batch 0] | D Loss: 0.6774 | G Loss: 1.2802\n",
            "Models saved for epoch 133.\n",
            "[Epoch 134/300] [Batch 0] | D Loss: 0.6807 | G Loss: 1.1500\n",
            "Models saved for epoch 134.\n",
            "[Epoch 135/300] [Batch 0] | D Loss: 0.6925 | G Loss: 1.1592\n",
            "Models saved for epoch 135.\n",
            "[Epoch 136/300] [Batch 0] | D Loss: 0.6954 | G Loss: 1.3127\n",
            "Models saved for epoch 136.\n",
            "[Epoch 137/300] [Batch 0] | D Loss: 0.6789 | G Loss: 1.2859\n",
            "Models saved for epoch 137.\n",
            "[Epoch 138/300] [Batch 0] | D Loss: 0.6874 | G Loss: 1.1710\n",
            "Models saved for epoch 138.\n",
            "[Epoch 139/300] [Batch 0] | D Loss: 0.6835 | G Loss: 1.6405\n",
            "Models saved for epoch 139.\n",
            "[Epoch 140/300] [Batch 0] | D Loss: 0.6710 | G Loss: 1.5127\n",
            "Models saved for epoch 140.\n",
            "[Epoch 141/300] [Batch 0] | D Loss: 0.6744 | G Loss: 1.5119\n",
            "Models saved for epoch 141.\n",
            "[Epoch 142/300] [Batch 0] | D Loss: 0.6834 | G Loss: 1.4222\n",
            "Models saved for epoch 142.\n",
            "[Epoch 143/300] [Batch 0] | D Loss: 0.6463 | G Loss: 1.7222\n",
            "Models saved for epoch 143.\n",
            "[Epoch 144/300] [Batch 0] | D Loss: 0.6113 | G Loss: 1.4504\n",
            "Models saved for epoch 144.\n",
            "[Epoch 145/300] [Batch 0] | D Loss: 0.5642 | G Loss: 1.5324\n",
            "Models saved for epoch 145.\n",
            "[Epoch 146/300] [Batch 0] | D Loss: 0.7090 | G Loss: 1.2936\n",
            "Models saved for epoch 146.\n",
            "[Epoch 147/300] [Batch 0] | D Loss: 0.6899 | G Loss: 1.3381\n",
            "Models saved for epoch 147.\n",
            "[Epoch 148/300] [Batch 0] | D Loss: 0.7006 | G Loss: 1.1656\n",
            "Models saved for epoch 148.\n",
            "[Epoch 149/300] [Batch 0] | D Loss: 0.6895 | G Loss: 1.2589\n",
            "Models saved for epoch 149.\n",
            "[Epoch 150/300] [Batch 0] | D Loss: 0.6930 | G Loss: 1.1568\n",
            "Models saved for epoch 150.\n",
            "[Epoch 151/300] [Batch 0] | D Loss: 0.7012 | G Loss: 1.2009\n",
            "Models saved for epoch 151.\n",
            "[Epoch 152/300] [Batch 0] | D Loss: 0.6942 | G Loss: 1.1264\n",
            "Models saved for epoch 152.\n",
            "[Epoch 153/300] [Batch 0] | D Loss: 0.6936 | G Loss: 1.0614\n",
            "Models saved for epoch 153.\n",
            "[Epoch 154/300] [Batch 0] | D Loss: 0.6927 | G Loss: 1.2021\n",
            "Models saved for epoch 154.\n",
            "[Epoch 155/300] [Batch 0] | D Loss: 0.6961 | G Loss: 1.2102\n",
            "Models saved for epoch 155.\n",
            "[Epoch 156/300] [Batch 0] | D Loss: 0.6952 | G Loss: 1.1742\n",
            "Models saved for epoch 156.\n",
            "[Epoch 157/300] [Batch 0] | D Loss: 0.6939 | G Loss: 1.1834\n",
            "Models saved for epoch 157.\n",
            "[Epoch 158/300] [Batch 0] | D Loss: 0.7003 | G Loss: 1.3803\n",
            "Models saved for epoch 158.\n",
            "[Epoch 159/300] [Batch 0] | D Loss: 0.7019 | G Loss: 1.2138\n",
            "Models saved for epoch 159.\n",
            "[Epoch 160/300] [Batch 0] | D Loss: 0.6980 | G Loss: 1.0228\n",
            "Models saved for epoch 160.\n",
            "[Epoch 161/300] [Batch 0] | D Loss: 0.6941 | G Loss: 1.1521\n",
            "Models saved for epoch 161.\n",
            "[Epoch 162/300] [Batch 0] | D Loss: 0.6927 | G Loss: 1.1645\n",
            "Models saved for epoch 162.\n",
            "[Epoch 163/300] [Batch 0] | D Loss: 0.6945 | G Loss: 1.2046\n",
            "Models saved for epoch 163.\n",
            "[Epoch 164/300] [Batch 0] | D Loss: 0.6923 | G Loss: 1.1356\n",
            "Models saved for epoch 164.\n",
            "[Epoch 165/300] [Batch 0] | D Loss: 0.6943 | G Loss: 1.0829\n",
            "Models saved for epoch 165.\n",
            "[Epoch 166/300] [Batch 0] | D Loss: 0.6935 | G Loss: 1.1796\n",
            "Models saved for epoch 166.\n",
            "[Epoch 167/300] [Batch 0] | D Loss: 0.7019 | G Loss: 1.1046\n",
            "Models saved for epoch 167.\n",
            "[Epoch 168/300] [Batch 0] | D Loss: 0.6869 | G Loss: 1.0996\n",
            "Models saved for epoch 168.\n",
            "[Epoch 169/300] [Batch 0] | D Loss: 0.7007 | G Loss: 1.1912\n",
            "Models saved for epoch 169.\n",
            "[Epoch 170/300] [Batch 0] | D Loss: 0.7040 | G Loss: 1.1456\n",
            "Models saved for epoch 170.\n",
            "[Epoch 171/300] [Batch 0] | D Loss: 0.6915 | G Loss: 1.1019\n",
            "Models saved for epoch 171.\n",
            "[Epoch 172/300] [Batch 0] | D Loss: 0.6919 | G Loss: 1.4470\n",
            "Models saved for epoch 172.\n",
            "[Epoch 173/300] [Batch 0] | D Loss: 0.6937 | G Loss: 1.1084\n",
            "Models saved for epoch 173.\n",
            "[Epoch 174/300] [Batch 0] | D Loss: 0.6971 | G Loss: 1.0966\n",
            "Models saved for epoch 174.\n",
            "[Epoch 175/300] [Batch 0] | D Loss: 0.7008 | G Loss: 1.1380\n",
            "Models saved for epoch 175.\n",
            "[Epoch 176/300] [Batch 0] | D Loss: 0.6938 | G Loss: 1.2185\n",
            "Models saved for epoch 176.\n",
            "[Epoch 177/300] [Batch 0] | D Loss: 0.6926 | G Loss: 1.1376\n",
            "Models saved for epoch 177.\n",
            "[Epoch 178/300] [Batch 0] | D Loss: 0.6921 | G Loss: 1.1698\n",
            "Models saved for epoch 178.\n",
            "[Epoch 179/300] [Batch 0] | D Loss: 0.6889 | G Loss: 1.1989\n",
            "Models saved for epoch 179.\n",
            "[Epoch 180/300] [Batch 0] | D Loss: 0.7208 | G Loss: 1.2379\n",
            "Models saved for epoch 180.\n",
            "[Epoch 181/300] [Batch 0] | D Loss: 0.6943 | G Loss: 1.1748\n",
            "Models saved for epoch 181.\n",
            "[Epoch 182/300] [Batch 0] | D Loss: 0.6829 | G Loss: 1.3111\n",
            "Models saved for epoch 182.\n",
            "[Epoch 183/300] [Batch 0] | D Loss: 0.6964 | G Loss: 1.0737\n",
            "Models saved for epoch 183.\n",
            "[Epoch 184/300] [Batch 0] | D Loss: 0.6899 | G Loss: 1.2491\n",
            "Models saved for epoch 184.\n",
            "[Epoch 185/300] [Batch 0] | D Loss: 0.6797 | G Loss: 1.2905\n",
            "Models saved for epoch 185.\n",
            "[Epoch 186/300] [Batch 0] | D Loss: 0.7026 | G Loss: 1.3064\n",
            "Models saved for epoch 186.\n",
            "[Epoch 187/300] [Batch 0] | D Loss: 0.6772 | G Loss: 1.2258\n",
            "Models saved for epoch 187.\n",
            "[Epoch 188/300] [Batch 0] | D Loss: 0.6926 | G Loss: 1.2604\n",
            "Models saved for epoch 188.\n",
            "[Epoch 189/300] [Batch 0] | D Loss: 0.6928 | G Loss: 1.2420\n",
            "Models saved for epoch 189.\n",
            "[Epoch 190/300] [Batch 0] | D Loss: 0.6933 | G Loss: 1.1365\n",
            "Models saved for epoch 190.\n",
            "[Epoch 191/300] [Batch 0] | D Loss: 0.6942 | G Loss: 1.0671\n",
            "Models saved for epoch 191.\n",
            "[Epoch 192/300] [Batch 0] | D Loss: 0.7058 | G Loss: 1.2144\n",
            "Models saved for epoch 192.\n",
            "[Epoch 193/300] [Batch 0] | D Loss: 0.6955 | G Loss: 1.2387\n",
            "Models saved for epoch 193.\n",
            "[Epoch 194/300] [Batch 0] | D Loss: 0.6923 | G Loss: 1.3562\n",
            "Models saved for epoch 194.\n",
            "[Epoch 195/300] [Batch 0] | D Loss: 0.6956 | G Loss: 1.0630\n",
            "Models saved for epoch 195.\n",
            "[Epoch 196/300] [Batch 0] | D Loss: 0.6998 | G Loss: 1.0508\n",
            "Models saved for epoch 196.\n",
            "[Epoch 197/300] [Batch 0] | D Loss: 0.6901 | G Loss: 1.1920\n",
            "Models saved for epoch 197.\n",
            "[Epoch 198/300] [Batch 0] | D Loss: 0.6943 | G Loss: 1.1637\n",
            "Models saved for epoch 198.\n",
            "[Epoch 199/300] [Batch 0] | D Loss: 0.6922 | G Loss: 1.1192\n",
            "Models saved for epoch 199.\n",
            "[Epoch 200/300] [Batch 0] | D Loss: 0.6941 | G Loss: 1.0480\n",
            "Models saved for epoch 200.\n",
            "[Epoch 201/300] [Batch 0] | D Loss: 0.6934 | G Loss: 1.4029\n",
            "Models saved for epoch 201.\n",
            "[Epoch 202/300] [Batch 0] | D Loss: 0.6915 | G Loss: 1.1450\n",
            "Models saved for epoch 202.\n",
            "[Epoch 203/300] [Batch 0] | D Loss: 0.6956 | G Loss: 1.1276\n",
            "Models saved for epoch 203.\n",
            "[Epoch 204/300] [Batch 0] | D Loss: 0.7089 | G Loss: 1.1755\n",
            "Models saved for epoch 204.\n",
            "[Epoch 205/300] [Batch 0] | D Loss: 0.6997 | G Loss: 1.0836\n",
            "Models saved for epoch 205.\n",
            "[Epoch 206/300] [Batch 0] | D Loss: 0.6920 | G Loss: 1.0448\n",
            "Models saved for epoch 206.\n",
            "[Epoch 207/300] [Batch 0] | D Loss: 0.6846 | G Loss: 1.2327\n",
            "Models saved for epoch 207.\n",
            "[Epoch 208/300] [Batch 0] | D Loss: 0.6942 | G Loss: 1.1790\n",
            "Models saved for epoch 208.\n",
            "[Epoch 209/300] [Batch 0] | D Loss: 0.6923 | G Loss: 1.0270\n",
            "Models saved for epoch 209.\n",
            "[Epoch 210/300] [Batch 0] | D Loss: 0.6926 | G Loss: 1.0284\n",
            "Models saved for epoch 210.\n",
            "[Epoch 211/300] [Batch 0] | D Loss: 0.6957 | G Loss: 1.1944\n",
            "Models saved for epoch 211.\n",
            "[Epoch 212/300] [Batch 0] | D Loss: 0.6878 | G Loss: 1.2037\n",
            "Models saved for epoch 212.\n",
            "[Epoch 213/300] [Batch 0] | D Loss: 0.7006 | G Loss: 1.2211\n",
            "Models saved for epoch 213.\n",
            "[Epoch 214/300] [Batch 0] | D Loss: 0.6976 | G Loss: 1.0811\n",
            "Models saved for epoch 214.\n",
            "[Epoch 215/300] [Batch 0] | D Loss: 0.7004 | G Loss: 1.1629\n",
            "Models saved for epoch 215.\n",
            "[Epoch 216/300] [Batch 0] | D Loss: 0.7027 | G Loss: 1.1163\n",
            "Models saved for epoch 216.\n",
            "[Epoch 217/300] [Batch 0] | D Loss: 0.6617 | G Loss: 1.1692\n",
            "Models saved for epoch 217.\n",
            "[Epoch 218/300] [Batch 0] | D Loss: 0.6280 | G Loss: 1.7851\n",
            "Models saved for epoch 218.\n",
            "[Epoch 219/300] [Batch 0] | D Loss: 0.6877 | G Loss: 1.2038\n",
            "Models saved for epoch 219.\n",
            "[Epoch 220/300] [Batch 0] | D Loss: 0.6991 | G Loss: 1.0789\n",
            "Models saved for epoch 220.\n",
            "[Epoch 221/300] [Batch 0] | D Loss: 0.6878 | G Loss: 1.1317\n",
            "Models saved for epoch 221.\n",
            "[Epoch 222/300] [Batch 0] | D Loss: 0.6960 | G Loss: 1.1104\n",
            "Models saved for epoch 222.\n",
            "[Epoch 223/300] [Batch 0] | D Loss: 0.6902 | G Loss: 1.0975\n",
            "Models saved for epoch 223.\n",
            "[Epoch 224/300] [Batch 0] | D Loss: 0.6901 | G Loss: 1.0762\n",
            "Models saved for epoch 224.\n",
            "[Epoch 225/300] [Batch 0] | D Loss: 0.6754 | G Loss: 1.2101\n",
            "Models saved for epoch 225.\n",
            "[Epoch 226/300] [Batch 0] | D Loss: 0.6547 | G Loss: 1.3269\n",
            "Models saved for epoch 226.\n",
            "[Epoch 227/300] [Batch 0] | D Loss: 0.6413 | G Loss: 1.5198\n",
            "Models saved for epoch 227.\n",
            "[Epoch 228/300] [Batch 0] | D Loss: 0.6545 | G Loss: 1.8524\n",
            "Models saved for epoch 228.\n",
            "[Epoch 229/300] [Batch 0] | D Loss: 0.6385 | G Loss: 1.6868\n",
            "Models saved for epoch 229.\n",
            "[Epoch 230/300] [Batch 0] | D Loss: 0.6894 | G Loss: 1.1196\n",
            "Models saved for epoch 230.\n",
            "[Epoch 231/300] [Batch 0] | D Loss: 0.6953 | G Loss: 1.0943\n",
            "Models saved for epoch 231.\n",
            "[Epoch 232/300] [Batch 0] | D Loss: 0.6880 | G Loss: 1.1548\n",
            "Models saved for epoch 232.\n",
            "[Epoch 233/300] [Batch 0] | D Loss: 0.6918 | G Loss: 1.1558\n",
            "Models saved for epoch 233.\n",
            "[Epoch 234/300] [Batch 0] | D Loss: 0.6892 | G Loss: 1.1249\n",
            "Models saved for epoch 234.\n",
            "[Epoch 235/300] [Batch 0] | D Loss: 0.6948 | G Loss: 1.0759\n",
            "Models saved for epoch 235.\n",
            "[Epoch 236/300] [Batch 0] | D Loss: 0.6908 | G Loss: 1.1767\n",
            "Models saved for epoch 236.\n",
            "[Epoch 237/300] [Batch 0] | D Loss: 0.6530 | G Loss: 1.2040\n",
            "Models saved for epoch 237.\n",
            "[Epoch 238/300] [Batch 0] | D Loss: 0.6128 | G Loss: 1.3923\n",
            "Models saved for epoch 238.\n",
            "[Epoch 239/300] [Batch 0] | D Loss: 0.6424 | G Loss: 1.7698\n",
            "Models saved for epoch 239.\n",
            "[Epoch 240/300] [Batch 0] | D Loss: 0.6435 | G Loss: 1.1705\n",
            "Models saved for epoch 240.\n",
            "[Epoch 241/300] [Batch 0] | D Loss: 0.6413 | G Loss: 1.3581\n",
            "Models saved for epoch 241.\n",
            "[Epoch 242/300] [Batch 0] | D Loss: 0.7044 | G Loss: 1.0607\n",
            "Models saved for epoch 242.\n",
            "[Epoch 243/300] [Batch 0] | D Loss: 0.6951 | G Loss: 1.1289\n",
            "Models saved for epoch 243.\n",
            "[Epoch 244/300] [Batch 0] | D Loss: 0.6989 | G Loss: 1.1561\n",
            "Models saved for epoch 244.\n",
            "[Epoch 245/300] [Batch 0] | D Loss: 0.7027 | G Loss: 1.0638\n",
            "Models saved for epoch 245.\n",
            "[Epoch 246/300] [Batch 0] | D Loss: 0.6981 | G Loss: 1.0552\n",
            "Models saved for epoch 246.\n",
            "[Epoch 247/300] [Batch 0] | D Loss: 0.6923 | G Loss: 1.1768\n",
            "Models saved for epoch 247.\n",
            "[Epoch 248/300] [Batch 0] | D Loss: 0.6999 | G Loss: 1.0198\n",
            "Models saved for epoch 248.\n",
            "[Epoch 249/300] [Batch 0] | D Loss: 0.6955 | G Loss: 1.0254\n",
            "Models saved for epoch 249.\n",
            "[Epoch 250/300] [Batch 0] | D Loss: 0.6933 | G Loss: 1.0526\n",
            "Models saved for epoch 250.\n",
            "[Epoch 251/300] [Batch 0] | D Loss: 0.6893 | G Loss: 1.0849\n",
            "Models saved for epoch 251.\n",
            "[Epoch 252/300] [Batch 0] | D Loss: 0.7139 | G Loss: 0.9915\n",
            "Models saved for epoch 252.\n",
            "[Epoch 253/300] [Batch 0] | D Loss: 0.6767 | G Loss: 1.1004\n",
            "Models saved for epoch 253.\n",
            "[Epoch 254/300] [Batch 0] | D Loss: 0.6542 | G Loss: 1.1758\n",
            "Models saved for epoch 254.\n",
            "[Epoch 255/300] [Batch 0] | D Loss: 0.6361 | G Loss: 1.3106\n",
            "Models saved for epoch 255.\n",
            "[Epoch 256/300] [Batch 0] | D Loss: 0.6530 | G Loss: 1.4032\n",
            "Models saved for epoch 256.\n",
            "[Epoch 257/300] [Batch 0] | D Loss: 0.6588 | G Loss: 1.6665\n",
            "Models saved for epoch 257.\n",
            "[Epoch 258/300] [Batch 0] | D Loss: 0.5952 | G Loss: 1.6964\n",
            "Models saved for epoch 258.\n",
            "[Epoch 259/300] [Batch 0] | D Loss: 0.6960 | G Loss: 1.1684\n",
            "Models saved for epoch 259.\n",
            "[Epoch 260/300] [Batch 0] | D Loss: 0.6934 | G Loss: 1.0681\n",
            "Models saved for epoch 260.\n",
            "[Epoch 261/300] [Batch 0] | D Loss: 0.6976 | G Loss: 1.1972\n",
            "Models saved for epoch 261.\n",
            "[Epoch 262/300] [Batch 0] | D Loss: 0.6918 | G Loss: 1.0854\n",
            "Models saved for epoch 262.\n",
            "[Epoch 263/300] [Batch 0] | D Loss: 0.6936 | G Loss: 0.9969\n",
            "Models saved for epoch 263.\n",
            "[Epoch 264/300] [Batch 0] | D Loss: 0.6950 | G Loss: 1.1110\n",
            "Models saved for epoch 264.\n",
            "[Epoch 265/300] [Batch 0] | D Loss: 0.6967 | G Loss: 1.1569\n",
            "Models saved for epoch 265.\n",
            "[Epoch 266/300] [Batch 0] | D Loss: 0.7024 | G Loss: 1.2566\n",
            "Models saved for epoch 266.\n",
            "[Epoch 267/300] [Batch 0] | D Loss: 0.6939 | G Loss: 1.0797\n",
            "Models saved for epoch 267.\n",
            "[Epoch 268/300] [Batch 0] | D Loss: 0.6497 | G Loss: 1.4392\n",
            "Models saved for epoch 268.\n",
            "[Epoch 269/300] [Batch 0] | D Loss: 0.6378 | G Loss: 1.3595\n",
            "Models saved for epoch 269.\n",
            "[Epoch 270/300] [Batch 0] | D Loss: 0.5919 | G Loss: 1.5688\n",
            "Models saved for epoch 270.\n",
            "[Epoch 271/300] [Batch 0] | D Loss: 0.6060 | G Loss: 1.3288\n",
            "Models saved for epoch 271.\n",
            "[Epoch 272/300] [Batch 0] | D Loss: 0.7054 | G Loss: 1.0796\n",
            "Models saved for epoch 272.\n",
            "[Epoch 273/300] [Batch 0] | D Loss: 0.6991 | G Loss: 1.0470\n",
            "Models saved for epoch 273.\n",
            "[Epoch 274/300] [Batch 0] | D Loss: 0.6926 | G Loss: 1.0683\n",
            "Models saved for epoch 274.\n",
            "[Epoch 275/300] [Batch 0] | D Loss: 0.6930 | G Loss: 1.0555\n",
            "Models saved for epoch 275.\n",
            "[Epoch 276/300] [Batch 0] | D Loss: 0.6930 | G Loss: 1.0922\n",
            "Models saved for epoch 276.\n",
            "[Epoch 277/300] [Batch 0] | D Loss: 0.6948 | G Loss: 1.1144\n",
            "Models saved for epoch 277.\n",
            "[Epoch 278/300] [Batch 0] | D Loss: 0.6933 | G Loss: 1.1202\n",
            "Models saved for epoch 278.\n",
            "[Epoch 279/300] [Batch 0] | D Loss: 0.6979 | G Loss: 1.0771\n",
            "Models saved for epoch 279.\n",
            "[Epoch 280/300] [Batch 0] | D Loss: 0.6894 | G Loss: 1.2227\n",
            "Models saved for epoch 280.\n",
            "[Epoch 281/300] [Batch 0] | D Loss: 0.6927 | G Loss: 1.0883\n",
            "Models saved for epoch 281.\n",
            "[Epoch 282/300] [Batch 0] | D Loss: 0.6935 | G Loss: 1.0152\n",
            "Models saved for epoch 282.\n",
            "[Epoch 283/300] [Batch 0] | D Loss: 0.6920 | G Loss: 1.0114\n",
            "Models saved for epoch 283.\n",
            "[Epoch 284/300] [Batch 0] | D Loss: 0.6932 | G Loss: 1.0251\n",
            "Models saved for epoch 284.\n",
            "[Epoch 285/300] [Batch 0] | D Loss: 0.6889 | G Loss: 1.1455\n",
            "Models saved for epoch 285.\n",
            "[Epoch 286/300] [Batch 0] | D Loss: 0.6921 | G Loss: 1.0994\n",
            "Models saved for epoch 286.\n",
            "[Epoch 287/300] [Batch 0] | D Loss: 0.6930 | G Loss: 1.0095\n",
            "Models saved for epoch 287.\n",
            "[Epoch 288/300] [Batch 0] | D Loss: 0.7004 | G Loss: 1.0494\n",
            "Models saved for epoch 288.\n",
            "[Epoch 289/300] [Batch 0] | D Loss: 0.6985 | G Loss: 1.0809\n",
            "Models saved for epoch 289.\n",
            "[Epoch 290/300] [Batch 0] | D Loss: 0.7017 | G Loss: 1.1545\n",
            "Models saved for epoch 290.\n",
            "[Epoch 291/300] [Batch 0] | D Loss: 0.6931 | G Loss: 1.1580\n",
            "Models saved for epoch 291.\n",
            "[Epoch 292/300] [Batch 0] | D Loss: 0.6952 | G Loss: 1.0641\n",
            "Models saved for epoch 292.\n",
            "[Epoch 293/300] [Batch 0] | D Loss: 0.6922 | G Loss: 1.0455\n",
            "Models saved for epoch 293.\n",
            "[Epoch 294/300] [Batch 0] | D Loss: 0.6857 | G Loss: 1.0534\n",
            "Models saved for epoch 294.\n",
            "[Epoch 295/300] [Batch 0] | D Loss: 0.6648 | G Loss: 1.2707\n",
            "Models saved for epoch 295.\n",
            "[Epoch 296/300] [Batch 0] | D Loss: 0.6658 | G Loss: 1.1714\n",
            "Models saved for epoch 296.\n",
            "[Epoch 297/300] [Batch 0] | D Loss: 0.6094 | G Loss: 1.3636\n",
            "Models saved for epoch 297.\n",
            "[Epoch 298/300] [Batch 0] | D Loss: 0.6362 | G Loss: 1.7910\n",
            "Models saved for epoch 298.\n",
            "[Epoch 299/300] [Batch 0] | D Loss: 0.6951 | G Loss: 1.0036\n",
            "Models saved for epoch 299.\n",
            "[Epoch 300/300] [Batch 0] | D Loss: 0.6926 | G Loss: 1.0907\n",
            "Models saved for epoch 300.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the Generator"
      ],
      "metadata": {
        "id": "LQYYlRoATMDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# Set generator to evaluation mode\n",
        "generator.eval()\n",
        "with torch.no_grad():\n",
        "    for i, (blurred, sharp) in enumerate(test_loader):\n",
        "        blurred, sharp = blurred.to(device), sharp.to(device)\n",
        "\n",
        "        # Generate fake (deblurred) images\n",
        "        fake_sharp = generator(blurred)\n",
        "\n",
        "        # Save results\n",
        "        save_image(blurred, f\"blurred_{i}.png\")\n",
        "        save_image(fake_sharp, f\"fake_sharp_{i}.png\")\n",
        "        save_image(sharp, f\"real_sharp_{i}.png\")\n",
        "        if i == 3:  # Save results for the first 10 batches\n",
        "            break"
      ],
      "metadata": {
        "id": "-Wos8FyaTOZw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}